{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data, strucuture and maths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import string\n",
    "import itertools\n",
    "from  more_itertools import unique_everseen\n",
    "import random\n",
    "import glob\n",
    "from ast import literal_eval\n",
    "\n",
    "#progress,performance and management\n",
    "from tqdm import tqdm_notebook\n",
    "import threading\n",
    "import os\n",
    "import ssl\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#pre-processing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#time\n",
    "import datetime as datetime\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "#text processing / regex\n",
    "import regex\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "#dataviz and look/feel\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "sns.set(style=\"white\", context=\"talk\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# language & NLP\n",
    "import spacy\n",
    "import langdetect as ld\n",
    "\n",
    "#network libraries and data viz\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "#plotly offline rendering\n",
    "from plotly.offline import download_plotlyjs, iplot, plot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "class InstagramGraph():\n",
    "\n",
    "    def __init__(self,csv,source_col='searched_for',post_col='post',user_col='user'):\n",
    "\n",
    "\n",
    "        self.df = pd.read_csv(csv,encoding='latin').head(750)\n",
    "\n",
    "        self.post_col = post_col\n",
    "\n",
    "        self.user_col = user_col\n",
    "\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "        self.lemma_count = 0\n",
    "\n",
    "        self.hashtag_count = 0\n",
    "\n",
    "        self.source = self.df[source_col].unique()\n",
    "\n",
    "        self.default_stopwords= ['photooftheday','picoftheday','like4likes',\n",
    "                                'like4like','instagood','likeforlikes',\n",
    "                                'l4l','likeforlike','instagram','follow4follow',\n",
    "                                'followforfollow','instadaily','instagrammers',\n",
    "                                'instalike','follow','likeforfollow',\n",
    "                                'like4follow','instamood','instafollow',\n",
    "                                'bestoftheday','like','followme','instapic'\n",
    "                                'repost','bhfyp']\n",
    "\n",
    "        self.getLanguage\n",
    "\n",
    "        self.cleaning\n",
    "\n",
    "        self.getHashtag\n",
    "\n",
    "        self.getHashtagLemma\n",
    "\n",
    "        self.getUserpostcount\n",
    "\n",
    "        self.getUserhashtagcount\n",
    "\n",
    "        self.getEdgesNodes\n",
    "\n",
    "        self.getGraph\n",
    "\n",
    "    #cleans and formats dataframe\n",
    "    def cleaning(self,df,col):\n",
    "\n",
    "        #drop nulls on post column\n",
    "        self.df.dropna(subset=[self.post_col],inplace=True)\n",
    "\n",
    "        #convert any posts to string\n",
    "        self.df[self.post_col] = self.df[self.post_col].map(lambda x: str(x))\n",
    "\n",
    "        #remove emojis\n",
    "        self.df[self.post_col] = self.df[self.post_col].map(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    #extracts hashtags from any string returning list of hashtags\n",
    "    def getHashtag(self,_string):\n",
    "\n",
    "        #splits string into list and appends unique hashtags into a new list\n",
    "        hashtags = [hashtag for hashtag in set([token for token in _string.split() if token.startswith(\"#\")])]\n",
    "\n",
    "        #if there are hashtags in the string we process them further..\n",
    "        if len(hashtags) > 0:\n",
    "\n",
    "            #this will break up any hashtags that haven't been seperated by a space\n",
    "            hashtags_seperated = [i for i in ''.join(hashtags).strip().split('#') if len(i) > 0]\n",
    "\n",
    "            #this will remove any punctuation\n",
    "            hashtags_clean = [hashtag.translate(str.maketrans('', '', string.punctuation)) for hashtag in hashtags_seperated]\n",
    "\n",
    "            hashtags_clean = [i.lower() for i in hashtags_clean]\n",
    "\n",
    "            #returns unique, cleaned hashtags without the\n",
    "            return list(set(hashtags_clean))\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    #converts list of strings to lemma (if applicable) returning list of lemmas\n",
    "    def getHashtagLemma(self,hashtags):\n",
    "\n",
    "        #create a spacy document using hashtags as an argument\n",
    "        doc = self.nlp(' '.join(hashtags))\n",
    "\n",
    "        #empty list for lemmas\n",
    "        tokens = []\n",
    "\n",
    "        #loop through each token,\n",
    "        for token in doc:\n",
    "\n",
    "            if token.lemma_ != '-PRON-':\n",
    "\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "                if str(token.text) != str(token.lemma_):\n",
    "\n",
    "                    self.lemma_count+=1\n",
    "\n",
    "        self.hashtag_count += len(tokens)\n",
    "\n",
    "        return list(set(tokens))\n",
    "\n",
    "    #gets a users post count that exists in the data\n",
    "    def getUserpostcount(self,user):\n",
    "\n",
    "        return self.user_count_dict[user]\n",
    "\n",
    "    #gets a users median hashtag use in the data\n",
    "    def getUserhashtagcount(self,user):\n",
    "\n",
    "        return self.user_hashtag_count_dict[user]\n",
    "\n",
    "    #gets the language of the string\n",
    "    def getLanguage(self,_string):\n",
    "\n",
    "        try:\n",
    "            return ld.detect(_string)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def eda(self):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Language split\n",
    "        \"\"\"\n",
    "        if self.translate == True:\n",
    "\n",
    "            language_frame = pd.DataFrame(list(self.language_split.items()))\n",
    "\n",
    "            language_frame.columns = ['language','incidence']\n",
    "\n",
    "            language_frame.incidence = language_frame.incidence.map(lambda x: x/sum(language_frame.incidence))\n",
    "\n",
    "            language_low_incidence = language_frame[language_frame.incidence < 0.05]\n",
    "\n",
    "            language_frame_summary = language_frame.replace(language_low_incidence.language.values,'other')\n",
    "\n",
    "            language_frame_summary = language_frame_summary.groupby('language')['incidence'].sum()\n",
    "\n",
    "            language_frame_summary.sort_values(ascending=False,inplace=True)\n",
    "\n",
    "            fig = go.Figure([go.Bar(x=language_frame_summary.index, y=language_frame_summary.values,name='Secondary Product')])\n",
    "\n",
    "            fig.update_layout(xaxis_tickangle=-45,\n",
    "\n",
    "                title=\"Incidence of language by post for #\"+self.source[0],\n",
    "\n",
    "                xaxis_title=\"Language\",\n",
    "\n",
    "                yaxis_title=\"Incidence\")\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "        def _histogram(metric,metric_label):\n",
    "\n",
    "            fig = go.Figure(data=[go.Histogram(x=metric,histnorm='probability density')])\n",
    "\n",
    "            fig.update_layout(\n",
    "\n",
    "                title=f\"Distribution of {metric_label} for #\"+self.source[0],\n",
    "\n",
    "                xaxis_title=f\"Count of {metric_label}\",\n",
    "\n",
    "                yaxis_title=\"Frequency\")\n",
    "\n",
    "            return fig.show()\n",
    "\n",
    "        _histogram(self.df.user_post_count,'user post frequency')\n",
    "\n",
    "        _histogram(self.df.hashtag_count,'hashtags by post')\n",
    "\n",
    "\n",
    "        #get each user's posting frequency\n",
    "        df_count = pd.DataFrame(self.df.user.value_counts())\n",
    "\n",
    "        #col labels\n",
    "        df_count.columns=['post_freq']\n",
    "\n",
    "        #normalise column\n",
    "        df_count['post_freq_norm'] = df_count['post_freq'].map(lambda x: int(x)/df_count['post_freq'].sum())\n",
    "\n",
    "        #cumulative sum on posts\n",
    "        cum_sum_posts = np.cumsum(df_count['post_freq_norm'])\n",
    "\n",
    "        users = []\n",
    "        count=1\n",
    "\n",
    "        for i in range(self.df.user.nunique()):\n",
    "            users.append(count)\n",
    "            count+=1\n",
    "\n",
    "        #normalise users\n",
    "        users_ = [i/users[-1] for i in users]\n",
    "\n",
    "        #growth = pd.DataFrame(zip(users,cum_sum_posts))\n",
    "        fig = go.Figure(data=go.Scatter(x=users_,y=cum_sum_posts))\n",
    "\n",
    "        fig.update_layout(title=f'User post contribution for #'+ self.source[0],\n",
    "\n",
    "                        xaxis_title='Normalised User Base',\n",
    "\n",
    "                        yaxis_title='Normalised Post Contribution')\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        return\n",
    "\n",
    "    #gets a list of hashtag lists from the dataset\n",
    "    def getBatches(self,additional_stopwords=[]):\n",
    "\n",
    "        #if no extra stopwords are specificed we use the defalut stop word list\n",
    "        if len(additional_stopwords) == 0:\n",
    "\n",
    "            self.current_stopwords=self.default_stopwords\n",
    "\n",
    "        #append new stopwords to default stopword list\n",
    "        else:\n",
    "            self.current_stopwords = self.default_stopwords+additional_stopwords\n",
    "\n",
    "        #function that iterates through list input and removes any stopwords\n",
    "        def _removestop(words):\n",
    "            for stop_word in self.current_stopwords:\n",
    "                try:\n",
    "                    words.remove(stop_word)\n",
    "                    words = words\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            return words\n",
    "\n",
    "        #apply function to hashtag column\n",
    "        df_nostop = self.target[self.target.columns[0]].map(_removestop)\n",
    "\n",
    "        #create new list of lists containing hashtags\n",
    "        batch = [[df_nostop.iloc[i]][0] for i in range(len(df_nostop.index))]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    #calculates the edges and nodes that exist in the list of hashtag lists\n",
    "    def getEdgesNodes(self,batches,min_frequency):\n",
    "\n",
    "        #ranks hashtags in alphabetical order\n",
    "        def _ranked_topics(batches):\n",
    "\n",
    "            batches.sort()\n",
    "\n",
    "            return batches\n",
    "\n",
    "        #finds all possible unique combinations of topics\n",
    "        def _unique_combinations(batches):\n",
    "            return list(itertools.combinations(_ranked_topics(batches), 2))\n",
    "\n",
    "        #adds each combination to a dictionary, if combination already exists value of key increases by one\n",
    "        def _add_unique_combinations(_unique_combinations,_dict):\n",
    "\n",
    "            for combination in _unique_combinations:\n",
    "\n",
    "                if combination in _dict:\n",
    "\n",
    "                    _dict[combination]+=1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    _dict[combination]=1\n",
    "\n",
    "            return _dict\n",
    "\n",
    "        edge_dict = {}\n",
    "\n",
    "        source = []\n",
    "\n",
    "        target = []\n",
    "\n",
    "        edge_frequency = []\n",
    "\n",
    "        #execute functions as above looping through each list, finding all unique combinations in each list\n",
    "        #and adding them to dict object\n",
    "        for batch in batches:\n",
    "\n",
    "            edge_dict = _add_unique_combinations(_unique_combinations(batch),edge_dict)\n",
    "\n",
    "        #create edge dataframe\n",
    "        for key,value in edge_dict.items():\n",
    "\n",
    "            source.append(key[0])\n",
    "\n",
    "            target.append(key[1])\n",
    "\n",
    "            edge_frequency.append(value)\n",
    "\n",
    "        edge_df = pd.DataFrame({'source':source,'target':target,'edge_frequency':edge_frequency})\n",
    "\n",
    "        edge_df.sort_values(by='edge_frequency',ascending=False,inplace=True)\n",
    "\n",
    "        edge_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        #mask edge dataframe, only retinaing edges that occur n times\n",
    "        edge_df = edge_df[edge_df['edge_frequency'] > min_frequency]\n",
    "\n",
    "        #create node dataframe\n",
    "        node_df = pd.DataFrame({'id':list(set(list(edge_df['source'])+list(edge_df['target'])))})\n",
    "\n",
    "        labels = [i for i in range(len(node_df['id']))]\n",
    "\n",
    "        node_df['id_code'] = node_df.index\n",
    "\n",
    "        #create a dictionary of all the nodes\n",
    "        node_dict = dict(zip(node_df['id'],labels))\n",
    "\n",
    "        #add relevant id's to each node in the edge dataframe\n",
    "        edge_df['source_code'] = edge_df['source'].apply(lambda x: node_dict[x])\n",
    "\n",
    "        edge_df['target_code'] = edge_df['target'].apply(lambda x: node_dict[x])\n",
    "\n",
    "        #retain some attributes for the instance\n",
    "        self.edge_df = edge_df\n",
    "\n",
    "        self.node_df = node_df\n",
    "\n",
    "        self.node_dict = node_dict\n",
    "\n",
    "        self.edge_dict = edge_dict\n",
    "\n",
    "        return\n",
    "\n",
    "    #build the graph using the edge and node data\n",
    "    def getGraph(self):\n",
    "\n",
    "        #function that loops through and appends edge tuples to list\n",
    "        def _extract_edges(edge_df):\n",
    "\n",
    "            tuple_out = []\n",
    "\n",
    "            for i in range(0,len(self.edge_df.index)):\n",
    "\n",
    "                tuple_out.append((self.edge_df['source_code'][i],self.edge_df['target_code'][i]))\n",
    "\n",
    "            return tuple_out\n",
    "\n",
    "        #instantiate an instance of a Networkx graph\n",
    "        G=nx.Graph()\n",
    "\n",
    "        #add the nodes to the instance\n",
    "        G.add_nodes_from(self.node_df.id_code)\n",
    "\n",
    "        #extract the edges\n",
    "        edge_tuples = _extract_edges(self.edge_df)\n",
    "\n",
    "        #loop through and add each edge to the instance\n",
    "        for i in edge_tuples:\n",
    "            G.add_edge(i[0],i[1])\n",
    "\n",
    "\n",
    "        return G\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Pipeline of all methods\n",
    "    \"\"\"\n",
    "    #generate all the features we need\n",
    "    def getFeatures(self,translate=False):\n",
    "\n",
    "        self.translate = translate\n",
    "\n",
    "        if self.translate == True:\n",
    "\n",
    "\n",
    "            print('Attempting to identify language...')\n",
    "\n",
    "            #detect language using getLanguage method\n",
    "            self.df['language'] = self.df[self.post_col].map(self.getLanguage)\n",
    "\n",
    "            #get language split as a class dictionary attribute\n",
    "            self.language_split = dict(self.df['language'].value_counts())\n",
    "            print('Languages identified...')\n",
    "\n",
    "        #call cleaning method\n",
    "        self.df = self.cleaning(self.df,self.post_col)\n",
    "        print('Data cleaned...')\n",
    "\n",
    "        print('Attempting to extract hashtags...')\n",
    "        #call getHashtag method to extract hashtags to new column\n",
    "        self.df['hashtags'] = self.df[self.post_col].map(self.getHashtag)\n",
    "\n",
    "        #drop any rows in the dataframe that don't have any hashtags\n",
    "        self.df.dropna(subset=['hashtags'],inplace=True)\n",
    "\n",
    "        #count of hashtags by post as new columns\n",
    "        self.df['hashtag_count'] = self.df['hashtags'].map(lambda x: len(x))\n",
    "\n",
    "        print('Attempting to lemmatise hashtags...')\n",
    "        #lemmatise any hashtags to new column\n",
    "        self.df['hashtags_lemma'] = self.df['hashtags'].map(self.getHashtagLemma)\n",
    "\n",
    "        lemma_conversion = self.lemma_count / self.hashtag_count\n",
    "\n",
    "        print(f'Of {str(self.hashtag_count)} hashtags, {str(self.lemma_count)} hashtags were successfully lemmatised ({str(lemma_conversion)})')\n",
    "\n",
    "        #get user post frequency as class attribute\n",
    "        self.user_count_dict = dict(self.df[self.user_col].value_counts())\n",
    "\n",
    "        #get median post count for each user as a class attribute\n",
    "        self.user_hashtag_count_dict = dict(self.df.groupby(self.user_col)['hashtag_count'].median())\n",
    "\n",
    "        #get user post count as new column\n",
    "        self.df['user_post_count'] = self.df[self.user_col].map(lambda x: self.getUserpostcount(x))\n",
    "\n",
    "        #get median user post count as new column\n",
    "        self.df['user_median_hashtag_count'] = self.df[self.user_col].map(lambda x: self.getUserhashtagcount(x))\n",
    "\n",
    "        print('Running EDA and generating plots...')\n",
    "\n",
    "        self.eda()\n",
    "\n",
    "        return\n",
    "\n",
    "    #select the data we want to include\n",
    "    def selectData(self,english=True,remove_verified=True,max_posts=3,lemma=True):\n",
    "\n",
    "        #retain some attributes\n",
    "        self._filterenglish = english\n",
    "\n",
    "        self._filterverified = remove_verified\n",
    "\n",
    "        self._filterpostcount = max_posts\n",
    "\n",
    "        self.df_edit = self.df.copy()\n",
    "\n",
    "        if self.translate == True:\n",
    "            #filter dataset to only include english if arg is true (default)\n",
    "            if self._filterenglish == True:\n",
    "\n",
    "                self.df_edit = self.df_edit[self.df_edit['language']=='en']\n",
    "\n",
    "        #filter dataset to only include unverified accounts if arg is true (default)\n",
    "        if self._filterverified == True:\n",
    "\n",
    "            self.df_edit = self.df_edit[self.df_edit['user_verified_status']== False]\n",
    "\n",
    "        #filter dataset to only include users who have posted under a threshold number of posts - gets rids of high volume posters\n",
    "        self.df_edit = self.df_edit[self.df_edit['user_post_count'] <= self._filterpostcount]\n",
    "\n",
    "        #retains the target column as an attribute of either hashtags that have been lemmatised or not\n",
    "        if lemma == True:\n",
    "\n",
    "            self.target = self.df_edit[['hashtags_lemma']]\n",
    "        else:\n",
    "            self.target = self.df_edit[['hashtags']]\n",
    "\n",
    "        print('Data Selected.')\n",
    "\n",
    "        return\n",
    "\n",
    "    #create edges and nodes and add these to an instance of a graph object\n",
    "    def buildGraph(self,additional_stopwords=[],min_frequency=5):\n",
    "\n",
    "        #call getBatches method passing any contextual stop words as an arg\n",
    "        batches = self.getBatches(additional_stopwords)\n",
    "\n",
    "        #call getEdgesNodes mnethod taking max frequency as an arg\n",
    "        self.getEdgesNodes(batches,min_frequency)\n",
    "\n",
    "        #call the getGraph method and build the graph\n",
    "        self.G = self.getGraph()\n",
    "        print('Graph successfully built.')\n",
    "        print('Node and Edge dataframes created.')\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        save a number of attributes to the instance of the class\n",
    "        \"\"\"\n",
    "        #retain graph object adjacencies\n",
    "        self.adjacencies = dict(self.G.adjacency())\n",
    "\n",
    "        #retain graph object node betweeness centrality\n",
    "        self.betweeness = nx.betweenness_centrality(self.G)\n",
    "\n",
    "        #retain graph object clustering coefficients\n",
    "        self.clustering_coeff = nx.clustering(self.G)\n",
    "\n",
    "        \"\"\"\n",
    "        add these attributes as columns on the node dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        self.node_df['adjacency_frequency'] = self.node_df['id_code'].map(lambda x: len(self.adjacencies[x]))\n",
    "\n",
    "        self.node_df['betweeness_centrality'] = self.node_df['id_code'].map(lambda x: self.betweeness[x])\n",
    "\n",
    "        self.node_df['clustering_coefficient'] = self.node_df['id_code'].map(lambda x: self.clustering_coeff[x])\n",
    "\n",
    "        #identify communities in instance of graph object and retain as attribute\n",
    "        self.communities = community.greedy_modularity_communities(self.G)\n",
    "\n",
    "        \"\"\"\n",
    "        assign each node to its community and add as column to node dataframe\n",
    "        \"\"\"\n",
    "        self.communities_dict = {}\n",
    "\n",
    "        nodes_in_community = [list(i) for i in self.communities]\n",
    "\n",
    "        for i in nodes_in_community:\n",
    "\n",
    "            self.communities_dict[nodes_in_community.index(i)] = i\n",
    "\n",
    "        def community_allocation(source_val):\n",
    "            for k,v in self.communities_dict.items():\n",
    "                if source_val in v:\n",
    "                    return k\n",
    "\n",
    "        self.node_df['community'] = self.node_df['id_code'].map(lambda x: community_allocation(x))\n",
    "\n",
    "        print('Communities calculated.')\n",
    "        return\n",
    "\n",
    "    #plot the graph using plotly\n",
    "    def plotGraph(self,sizing=75,node_size='adjacency_frequency',layout=nx.kamada_kawai_layout,light_theme=True,colorscale='Viridis',community_plot=False):\n",
    "\n",
    "        #formatting options for plot - dark vs. light theme\n",
    "        if light_theme:\n",
    "            back_col = '#ffffff'\n",
    "            edge_col = '#ece8e8'\n",
    "\n",
    "        else:\n",
    "            back_col = '#000000'\n",
    "            edge_col = '#2d2b2b'\n",
    "\n",
    "        \"\"\"\n",
    "        normalise all graph metrics\n",
    "        \"\"\"\n",
    "        #subset graph metrics\n",
    "        X = self.node_df[self.node_df.columns[2:5]]\n",
    "\n",
    "        #get columns labels\n",
    "        cols = self.node_df.columns[2:5]\n",
    "\n",
    "        #instantiate instance of MinMaxScaler class\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        #transform graph metrics\n",
    "        X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "        #create new dataframe of scaled metrics\n",
    "        plot_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "        plot_df.columns=cols\n",
    "\n",
    "        for i in plot_df.columns:\n",
    "            plot_df[i] = plot_df[i].apply(lambda x: x*sizing)\n",
    "\n",
    "\n",
    "        #extract graph x,y co-ordinates from G instance\n",
    "        pos = layout(self.G)\n",
    "\n",
    "        #add position of each node from G to 'pos' key\n",
    "        for node in self.G.nodes:\n",
    "            self.G.nodes[node]['pos'] = list(pos[node])\n",
    "\n",
    "\n",
    "\n",
    "        stack = []\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        #add edges to Plotly go.Scatter object\n",
    "        for edge in self.G.edges:\n",
    "\n",
    "            x0, y0 = self.G.nodes[edge[0]]['pos']\n",
    "\n",
    "            x1, y1 = self.G.nodes[edge[1]]['pos']\n",
    "\n",
    "            weight = 0.5\n",
    "\n",
    "            trace = go.Scatter(x=tuple([x0, x1, None]), y=tuple([y0, y1, None]),\n",
    "                               mode='lines',\n",
    "                               line={'width': weight},\n",
    "                               marker=dict(color=edge_col),\n",
    "                               line_shape='spline',\n",
    "                               opacity=1)\n",
    "\n",
    "            #append edge traces\n",
    "            stack.append(trace)\n",
    "\n",
    "            index = index + 1\n",
    "\n",
    "        #conditionals for either showing a plot where formatting denotes community or not\n",
    "        if community_plot == True:\n",
    "\n",
    "            #make a partly empty dictionary for the nodes\n",
    "            marker = {'size':[],'line':dict(width=0.5,color=edge_col),'color':[]}\n",
    "\n",
    "        else:\n",
    "\n",
    "            #make a partly empty dictionary for the nodes\n",
    "            marker = {'colorscale':colorscale,'size':[],'line':dict(width=0.5,color=edge_col),'color':[],'colorbar':dict(thickness=15,\n",
    "                                                                                               title='Node Connections',\n",
    "                                                                                               xanchor='left',\n",
    "                                                                                               titleside='right')}\n",
    "\n",
    "\n",
    "        #initialise a go.Scatter object for the nodes\n",
    "        node_trace = go.Scatter(x=[], y=[], hovertext=[], text=[], mode='markers', textposition=\"bottom center\",\n",
    "                                hoverinfo=\"text\", marker=marker)\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        #add nodes to Plotly go.Scatter object\n",
    "        for node in self.G.nodes():\n",
    "\n",
    "            x, y = self.G.nodes[node]['pos']\n",
    "\n",
    "            node_trace['x'] += tuple([x])\n",
    "\n",
    "            node_trace['y'] += tuple([y])\n",
    "\n",
    "            node_trace['text'] += tuple([self.node_df['id'][index]])\n",
    "\n",
    "            if community_plot == True:\n",
    "\n",
    "                node_trace['marker']['color'] += tuple(list(self.node_df.community))\n",
    "\n",
    "                node_trace['marker']['size'] += tuple([list(plot_df[node_size])[index]])\n",
    "\n",
    "            else:\n",
    "\n",
    "                node_trace['marker']['color'] += tuple([list(self.node_df.adjacency_frequency)[index]])\n",
    "\n",
    "                node_trace['marker']['size'] += tuple([list(self.node_df.adjacency_frequency)[index]])\n",
    "\n",
    "            index = index + 1\n",
    "\n",
    "        #append node traces\n",
    "        stack.append(node_trace)\n",
    "\n",
    "\n",
    "        #set up axis for plot\n",
    "        axis=dict(showline=False, # hide axis line, grid, ticklabels and  title\n",
    "                  zeroline=False,\n",
    "                  showgrid=False,\n",
    "                  showticklabels=False,\n",
    "                  title=''\n",
    "                  )\n",
    "\n",
    "        #set up figure for plot\n",
    "        figure = {\n",
    "        \"data\": stack,\n",
    "        \"layout\":\n",
    "        go.Layout(title=str(self.source[0]+' is..'),\n",
    "                    font= dict(family='Arial',size=20),\n",
    "                    width=1100,\n",
    "                    height=1100,\n",
    "                    autosize=False,\n",
    "                    showlegend=False,\n",
    "                    xaxis=axis,\n",
    "                    yaxis=axis,\n",
    "                    margin=dict(\n",
    "                    l=40,\n",
    "                    r=40,\n",
    "                    b=85,\n",
    "                    t=100,\n",
    "                    pad=0,\n",
    "\n",
    "            ),\n",
    "            hovermode='closest',\n",
    "            plot_bgcolor=back_col, #set background color\n",
    "            )}\n",
    "\n",
    "        #retain plot figure as attribute\n",
    "        self.graph_plot = figure\n",
    "\n",
    "        #plot the figure\n",
    "        iplot(self.graph_plot)\n",
    "\n",
    "        return\n",
    "\n",
    "    #sunburst that plots communities and relevant hahstags\n",
    "    def plotCommunity(self,colorscale=False):\n",
    "\n",
    "        #make copy of node dataframe\n",
    "        df_temp = self.node_df.copy()\n",
    "\n",
    "        #change community label to string (needed for plot)\n",
    "        df_temp['community'] = df_temp['community'].map(lambda x: str(x))\n",
    "\n",
    "        #conditionals for plot type\n",
    "        if colorscale == False:\n",
    "\n",
    "            fig = px.sunburst(df_temp, path=['community', 'id'], values='adjacency_frequency',color='community',hover_name=None,\n",
    "                          hover_data=None)\n",
    "        else:\n",
    "            fig = px.sunburst(df_temp, path=['community', 'id'], values='adjacency_frequency',\n",
    "                          color='betweeness_centrality', hover_data=None,\n",
    "                          color_continuous_scale='blugrn',\n",
    "                          color_continuous_midpoint=np.average(df_temp['betweeness_centrality'], weights=df_temp['betweeness_centrality']))\n",
    "\n",
    "        #add margin to plot\n",
    "        fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n",
    "\n",
    "        #retain community plot as attribute\n",
    "        self.community_plot = fig\n",
    "\n",
    "        #offline sunburst plot\n",
    "        iplot(self.community_plot)\n",
    "\n",
    "        return\n",
    "\n",
    "    #save map / sunburst plot locally as html file\n",
    "    def savePlot(self,plot='map'):\n",
    "\n",
    "        #get current time\n",
    "        date = str(pd.to_datetime(datetime.datetime.now())).split(' ')[0]\n",
    "\n",
    "\n",
    "        if plot == 'map':\n",
    "\n",
    "            plot_save = self.graph_plot\n",
    "\n",
    "            filename=date+'_'+self.source[0]+'_graph_plot_instagram.html'\n",
    "\n",
    "            plotly.offline.plot(plot_save, filename=filename)\n",
    "\n",
    "\n",
    "        elif plot == 'community':\n",
    "\n",
    "            plot_save = self.community_plot\n",
    "\n",
    "            filename= date+'_'+self.source[0]+'_community_plot_instagram.html'\n",
    "\n",
    "            plotly.offline.plot(plot_save, filename=filename)\n",
    "\n",
    "        return print('Plot saved.')\n",
    "\n",
    "    #save csv output\n",
    "    def saveTables(self):\n",
    "\n",
    "        date = str(pd.to_datetime(datetime.datetime.now())).split(' ')[0]\n",
    "\n",
    "        self.node_df.to_csv(date+\"_node_df_\"+str(self.source[0])+\".csv\",index=False)\n",
    "        print('Saved nodes')\n",
    "\n",
    "        self.edge_df.to_csv(date+\"_edge_df_\"+str(self.source[0])+\".csv\",index=False)\n",
    "        print('Saved edges')\n",
    "\n",
    "        self.df_edit.to_csv(date+\"_df_edit_\"+str(self.source[0])+\".csv\",index=False)\n",
    "        print('Saved edited dataframe')\n",
    "\n",
    "        self.df.to_csv(date+\"_df_\"+str(self.source[0])+\"_.csv\",index=False)\n",
    "        print('Saved unedited dataframe')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'human_connection_crowdDNA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0edff31d537d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'human_connection_crowdDNA.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'human_connection_crowdDNA.csv'"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('human_connection_crowdDNA.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
