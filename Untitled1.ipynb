{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#requests\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "#data, strucuture and maths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import string\n",
    "from  more_itertools import unique_everseen\n",
    "import random\n",
    "\n",
    "#progress,performance and management\n",
    "from tqdm import tqdm_notebook\n",
    "import threading\n",
    "import os\n",
    "import ssl\n",
    "from IPython.display import clear_output\n",
    "import platform\n",
    "import os\n",
    "\n",
    "# imports used in Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#time\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "#text processing / regex\n",
    "import regex\n",
    "import re\n",
    "\n",
    "#make wide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#passwords\n",
    "import getpass\n",
    "\n",
    "\n",
    "\n",
    "class InstagramScraper():\n",
    "\n",
    "    \"\"\"\n",
    "    Class that allows you to scrape the content of Instagram posts, either\n",
    "    a profile or a hashtag.\n",
    "    Initialised with the location of your Chromedriver location\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,driver_loc='/usr/lib/chromium-browser/chromedriver'):\n",
    "\n",
    "        self.driver_loc = driver_loc\n",
    "\n",
    "    def multithreadCompile(self,thread_count,iteration_list,func):\n",
    "\n",
    "        \"\"\"\n",
    "        This function compiles the batched needed for mult-threadding\n",
    "        Args:\n",
    "            thread_count is the number of threads used for multi-threadding\n",
    "            iteration_list is the source list of urls to iterate over\n",
    "            func is the function to be used in the multi-thredding process\n",
    "        Returns:\n",
    "            The batches that have been allocated to be run using the specified\n",
    "            function\n",
    "        \"\"\"\n",
    "\n",
    "        jobs = [] #empty list for jobs\n",
    "\n",
    "        #distribute iteration list to batches and append to jobs list\n",
    "        batches = [i.tolist() for i in np.array_split(iteration_list,thread_count)]\n",
    "\n",
    "        for i in range(len(batches)):\n",
    "\n",
    "            jobs.append(threading.Thread(target=func,args=[batches[i]]))\n",
    "\n",
    "        return jobs\n",
    "\n",
    "    def multithreadExecute(self,jobs):\n",
    "\n",
    "        \"\"\"\n",
    "        This function executes the multi-threadding process\n",
    "        Args:\n",
    "            The batches that have been appended to a jobs list\n",
    "        Returns:\n",
    "            Nothing, merely executes the multi-threadding\n",
    "        \"\"\"\n",
    "\n",
    "        # Start the threads\n",
    "        for j in jobs:\n",
    "            print('execute working')\n",
    "            j.start()\n",
    "\n",
    "        # Ensure all of the threads have finished\n",
    "        for j in jobs:\n",
    "\n",
    "            j.join()\n",
    "\n",
    "        return\n",
    "\n",
    "    def getJson(self,url):\n",
    "\n",
    "        \"\"\"\n",
    "        This function exracts a JSON style dictionary from the html for any\n",
    "        given unique Instagram post\n",
    "        Args:\n",
    "            An Instagram post URL\n",
    "        Returns:\n",
    "            JSON dictionary ouput\n",
    "        \"\"\"\n",
    "\n",
    "        page = urlopen(url).read() #read url\n",
    "\n",
    "        data=BeautifulSoup(page, 'html.parser') #get a BeautifulSoup object\n",
    "\n",
    "        body = data.find('body') #find body element\n",
    "\n",
    "        script = body.find('script') #find script element\n",
    "\n",
    "        #some string formatting\n",
    "        raw = script.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "\n",
    "        #load string\n",
    "        json_data=json.loads(raw)\n",
    "\n",
    "        return json_data #return JSON dictonary\n",
    "\n",
    "    def userDetails(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Functions that capture log in details and logs user into Instagram\n",
    "        Args:\n",
    "            None needed\n",
    "        Returns:\n",
    "            Nothing\n",
    "        \"\"\"\n",
    "        #capture username\n",
    "        username = input('Enter username...')\n",
    "\n",
    "        #capture password\n",
    "        password = getpass.getpass('Enter password...')\n",
    "\n",
    "        self._password = password #retain password as attribute\n",
    "\n",
    "        self._username = username #retain user name as attribute\n",
    "\n",
    "        return\n",
    "\n",
    "    def openWebdriver(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Launches Chrome webdriver\n",
    "        Args:\n",
    "            None needed\n",
    "        Returns:\n",
    "            driver\n",
    "        \"\"\"\n",
    "\n",
    "        #intiate driver\n",
    "        print(\"Launching driver...\")\n",
    "\n",
    "        #retain current driver as attribute\n",
    "        driver = webdriver.Chrome(self.driver_loc)\n",
    "\n",
    "        return driver\n",
    "\n",
    "    def closeWebdriver(self,driver):\n",
    "\n",
    "        \"\"\"\n",
    "        Closes Chrome webdriver\n",
    "        Args:\n",
    "            webDriver\n",
    "        Returns:\n",
    "            Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def instagramLogin(self,driver):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs in to Instagram\n",
    "        Args:\n",
    "            Current webdriver\n",
    "        Returns:\n",
    "            Current webdriver - logged into Instagram\n",
    "        \"\"\"\n",
    "\n",
    "        #base url\n",
    "        driver.get('https://www.instagram.com/accounts/login/?source=auth_switcher')\n",
    "\n",
    "        sleep(2) #wait\n",
    "\n",
    "        #log in\n",
    "        username_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input')\n",
    "\n",
    "        username_field.click() #click on username button\n",
    "\n",
    "        #send username\n",
    "        username_field.send_keys(self._username)\n",
    "\n",
    "        #locate element to click\n",
    "        try:\n",
    "            password_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input')\n",
    "\n",
    "        except:\n",
    "            password_field = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/div/label/input')\n",
    "\n",
    "        password_field.click()\n",
    "\n",
    "        password_field.send_keys(self._password)\n",
    "\n",
    "        sleep(2)\n",
    "\n",
    "        #find log in button\n",
    "        login_button = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]')\n",
    "\n",
    "        login_button.click()\n",
    "\n",
    "        sleep(3)\n",
    "\n",
    "        #locate floating window to click and close\n",
    "#         floating_window = driver.find_element_by_class_name('piCib')\n",
    "\n",
    "#         button = floating_window.find_element_by_class_name('mt3GC')\n",
    "\n",
    "#         not_now = button.find_element_by_xpath('/html/body/div[4]/div/div/div[3]/button[2]')\n",
    "\n",
    "#         not_now.click()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    def setTarget(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that sets either a profile or a hashtag as a target\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            base url to scrape - either a hashtag page or a profile page\n",
    "        \"\"\"\n",
    "\n",
    "        #tou can choose either hashtag search or a profile to search\n",
    "        route = input('What do you want to scrape, profile posts or hashtags? (p/h)')\n",
    "\n",
    "        #if hashtags\n",
    "        if route == 'h':\n",
    "\n",
    "            #set hashtag\n",
    "            hashtag = input('Which hashtag do you want to scrape posts for: ')\n",
    "\n",
    "            self.target_label = '#'+hashtag #retain hashtag as attribute\n",
    "\n",
    "            tag_url = 'https://www.instagram.com/explore/tags/' #set base url\n",
    "\n",
    "            self._target = tag_url+hashtag #set url to scrape from\n",
    "\n",
    "            return self._target #return url to scrape from\n",
    "\n",
    "        else:\n",
    "\n",
    "            profile = input('What profile do you want to scrape posts for: ')\n",
    "\n",
    "            self.target_label = '@'+profile #retain profile as attribute\n",
    "\n",
    "            profile_url = 'https://www.instagram.com/' #set base url\n",
    "\n",
    "            self._target = profile_url+profile #set url to scrape from\n",
    "\n",
    "            return self._target #return url to scrape from\n",
    "\n",
    "    def scrapeLinks(self,url):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that scrapes the links needed\n",
    "        Args:\n",
    "            target_url\n",
    "        Returns:\n",
    "            Nothing - but retains a list of urls to scrape\n",
    "        \"\"\"\n",
    "\n",
    "        #pass url as argument to Selenium webDriver, loads url\n",
    "        self.activedriver.get(url)\n",
    "\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        #start maximised\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "\n",
    "        #gets scroll height\n",
    "        last_height = self.activedriver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        #initiate empty list for unique Instagram links\n",
    "        links = []\n",
    "\n",
    "        #some lines for user interactivity / selection of link target(n)\n",
    "        print(\"\\n\")\n",
    "        target = input(\"How many links do you want to scrape (minimum)?: \")\n",
    "        print(\"\\n\")\n",
    "        print(\"Staring Selenium scrape, please keep browser open.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #this loops round until n links achieved or page has ended\n",
    "\n",
    "        while True:\n",
    "\n",
    "            source = self.activedriver.page_source\n",
    "\n",
    "            data= BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "            body = data.find('body')\n",
    "\n",
    "            #script = body.find('span')\n",
    "\n",
    "            for link in body.findAll('a'):\n",
    "\n",
    "                if re.match(\"/p\", link.get('href')):\n",
    "\n",
    "                    links.append('https://www.instagram.com'+link.get('href'))\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Scroll down to bottom\n",
    "            self.activedriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.activedriver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            #if no more content, scrape loop is terminated\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "\n",
    "            #update on successful links scraped\n",
    "            print(\"Scraped \", len(links),\" links, \", len(set(links)),' are unique')\n",
    "\n",
    "            #if n target met then while loop breaks\n",
    "            if len(set(links))>int(target):\n",
    "                break\n",
    "\n",
    "        #links are saved as an attribute for the class instance\n",
    "        self._links = list(unique_everseen(links))\n",
    "\n",
    "        #clear the screen and provide user feedback on performance\n",
    "        clear_output()\n",
    "\n",
    "        print(\"Finished scraping links. Maxed out at \", len(links),\" links, of which \", len(self._links),' are unique.')\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"Unique links obtained. Closing driver\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # close driver\n",
    "        self.closeWebdriver(self.activedriver)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def postDate(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the date of post\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            datetime of post\n",
    "        \"\"\"\n",
    "\n",
    "        return datetime.utcfromtimestamp(data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['taken_at_timestamp']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    def postUser(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the username of the person who posted\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            username\n",
    "        \"\"\"\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['owner']['username']\n",
    "\n",
    "    def postVerifiedUser(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function gets the verified status of the user\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            verified status\n",
    "        \"\"\"\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['owner']['is_verified']\n",
    "\n",
    "\n",
    "    def postLikes(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the number of likes the post received\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            number of likes\n",
    "        \"\"\"\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_preview_like']['count']\n",
    "\n",
    "    def postVerifiedTags(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the verified tags that a post contains\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            the verified tags in the post\n",
    "        \"\"\"\n",
    "\n",
    "        tag_end_point = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_tagged_user']['edges']\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        verif = []\n",
    "\n",
    "        for i in range(len(tag_end_point)):\n",
    "\n",
    "            entities.append(tag_end_point[i]['node']['user']['full_name'])\n",
    "\n",
    "            verif.append(tag_end_point[i]['node']['user']['is_verified'])\n",
    "\n",
    "        df = pd.DataFrame({'Brand':entities,'Verified':verif})\n",
    "\n",
    "        df = df[df.Verified == True]\n",
    "\n",
    "        if len(list(df.Brand)) < 1:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "        else:\n",
    "\n",
    "            return list(df.Brand)\n",
    "\n",
    "    def postUnverifiedTags(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the unverified tags a post contains\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            the unverified tags in the post\n",
    "        \"\"\"\n",
    "\n",
    "        tag_end_point = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_tagged_user']['edges']\n",
    "\n",
    "        tags = [] #emoty list for entities\n",
    "\n",
    "        verif = [] #empty list for verified status\n",
    "\n",
    "        #loop through\n",
    "        for i in range(len(tag_end_point)):\n",
    "\n",
    "            #append entities\n",
    "            tags.append(tag_end_point[i]['node']['user']['full_name'])\n",
    "\n",
    "            #append verified status\n",
    "            verif.append(tag_end_point[i]['node']['user']['is_verified'])\n",
    "\n",
    "        #DataFrame of verified / unverified tags\n",
    "        df = pd.DataFrame({'Tag':tags,'Verified':verif})\n",
    "\n",
    "        #subset on unverified tags\n",
    "        df = df[df.Verified == False]\n",
    "\n",
    "        #if there are unverified tags then return NaN else return unverified tags\n",
    "        if len(list(df.Tag)) < 1:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "        else:\n",
    "\n",
    "            return ''.join(list(df.Tag))\n",
    "\n",
    "    def postComment(self,data):\n",
    "\n",
    "        return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "\n",
    "    #get location of post\n",
    "    def postLocation(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the post location if available\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            the posts location\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            if len(list(data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['location']['name'])) > 0:\n",
    "\n",
    "                return data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['location']['name']\n",
    "        except:\n",
    "\n",
    "            return np.nan\n",
    "\n",
    "    #get accessibility  / image data\n",
    "    def postAccessibility(self,data):\n",
    "\n",
    "        \"\"\"\n",
    "        Function that gets the post accessibility data if available\n",
    "        Args:\n",
    "            JSON dictionary for post\n",
    "        Returns:\n",
    "            the accessibility data\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                image = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['accessibility_caption'].replace('Image may contain: ','').replace(' and ',', ').replace('one or more ','')\n",
    "\n",
    "                return image\n",
    "\n",
    "            except:\n",
    "                image = data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_sidecar_to_children']['edges'][0]['node']['accessibility_caption'].replace('Image may contain: ','').replace(' and ',', ').replace('one or more ','')\n",
    "\n",
    "                return image\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    #return original post link\n",
    "    def postLink(self,data):\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The three main methods that combine all above\n",
    "    \"\"\"\n",
    "    #get user details, log in and initiate driver\n",
    "    def logIn(self):\n",
    "\n",
    "        self.userDetails()\n",
    "\n",
    "        driver = self.openWebdriver()\n",
    "\n",
    "        self.activedriver = self.instagramLogin(driver)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        print('Successfully logged in..ready to scrape')\n",
    "\n",
    "    #get all the unique links\n",
    "    def getLinks(self):\n",
    "\n",
    "        return self.scrapeLinks(self.setTarget())\n",
    "\n",
    "    #extract data and return dataframe\n",
    "    def getData(self):\n",
    "\n",
    "        #create empty lists for posts and comments\n",
    "        post_date_l = []\n",
    "\n",
    "        post_user_l = []\n",
    "\n",
    "        post_verif_l = []\n",
    "\n",
    "        post_likes_l = []\n",
    "\n",
    "        post_tags_v_l =[]\n",
    "\n",
    "        post_tags_u_l = []\n",
    "\n",
    "        post_l = []\n",
    "\n",
    "        post_location_l = []\n",
    "\n",
    "        post_insta_classifier_l = []\n",
    "\n",
    "        post_link_l = []\n",
    "\n",
    "        self._listStack = [post_date_l,post_user_l,post_verif_l,post_likes_l,post_tags_v_l,\n",
    "                      post_tags_u_l,post_l,post_location_l,post_insta_classifier_l,post_link_l]\n",
    "\n",
    "        self._functionStack = [ self.postDate,\n",
    "                                    self.postUser,\n",
    "                                    self.postVerifiedUser,\n",
    "                                    self.postLikes,\n",
    "                                    self.postVerifiedTags,\n",
    "                                    self.postUnverifiedTags,\n",
    "                                    self.postComment,\n",
    "                                    self.postLocation,\n",
    "                                    self.postAccessibility,\n",
    "                                    self.postLink]\n",
    "\n",
    "        def extractData(links=self._links):\n",
    "\n",
    "            #loops through and calls each data collection function on each link\n",
    "            for i in tqdm_notebook(range(len(links))):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    data = self.getJson(links[i])\n",
    "\n",
    "                    for function in self._functionStack:\n",
    "\n",
    "                        if function != self._functionStack[-1]:\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                self._listStack[self._functionStack.index(function)].append(function(data))\n",
    "\n",
    "                            except:\n",
    "\n",
    "                                 self._listStack[self._functionStack.index(function)].append(np.nan)\n",
    "                        else:\n",
    "                            self._listStack[-1].append(self._functionStack[-1](links[i]))\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "            return\n",
    "\n",
    "        # execute html parsing fuction using multi threading\n",
    "        print(\"Attemping multi-threading...\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        threads = int(input(\"How many threads?: \"))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"Executing...\")\n",
    "\n",
    "        self.multithreadExecute(self.multithreadCompile(threads,self._links,extractData))\n",
    "\n",
    "        #set up intial data structure\n",
    "        df = pd.DataFrame({'searched_for':[self.target_label]*len(post_l),\n",
    "                           'post_link' :post_link_l,\n",
    "                           'post_date':post_date_l,\n",
    "                           'post':post_l,\n",
    "                           'user':post_user_l,\n",
    "                           'user_verified_status': post_verif_l,\n",
    "                           'post_likes':post_likes_l,\n",
    "                           'post_verified_tags':post_tags_v_l,\n",
    "                           'post_unverified_tags':post_tags_u_l,\n",
    "                           'post_location':post_location_l,\n",
    "                           'post_image':post_insta_classifier_l,\n",
    "\n",
    "                               })\n",
    "\n",
    "        df.sort_values(by='post_date',ascending=False,inplace=True)\n",
    "\n",
    "        df.reset_index(drop=True,inplace=True) #reset index\n",
    "\n",
    "        self._df = df #retain final DataFrame as attribute\n",
    "        print(df)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "togetherness = InstagramScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in..ready to scrape\n"
     ]
    }
   ],
   "source": [
    "togetherness.logIn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping links. Maxed out at  780  links, of which  201  are unique.\n",
      "\n",
      "\n",
      "Unique links obtained. Closing driver\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "togetherness.getLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getData'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4148c75d779c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getData'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attemping multi-threading...\n",
      "\n",
      "\n",
      "How many threads?: 20\n",
      "\n",
      "\n",
      "Executing...\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n",
      "execute working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/ipykernel_launcher.py:618: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0d77ed884e4797a286e39a502da9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute working\n",
      "execute working\n",
      "execute working\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1328924f5447443e9e53cb78320b68b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcac1dea3d784437a54198456d450de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb1d0c9b4bc48509a86bf35c42f6bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333f55ef835e4ceb8fc3f8c33f37f8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb0c1e837b14b5b84949f38b79c2932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db50287249a4a8698f205c51cef1732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4fc34f5ec042aea87496a593973793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0899d8771794db3a3b6ec444f0be7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aab11f6f32c4a91991b7003cf1443c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fba5b419d6541bbbcdf717de1c3db93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c4e9f49c2417e85eef546e907805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6697c02fe07a4d84becc775d1348cb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3573e68bf0f14823a053d9fce38991f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dccb7d6007b4f4ab0a3a0c683b8b99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd313fff3dba45ebb70e6e15b529b60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e453d80cf44020b18299bba51e8548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef281f6185bb4e6189bc14980a79ac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbecc65e91d04860885ddc1be8656ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ed8bc4a4e1483e953f782dc05ed5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [searched_for, post_link, post_date, post, user, user_verified_status, post_likes, post_verified_tags, post_unverified_tags, post_location, post_image]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "togetherness_df = togetherness.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>searched_for</th>\n",
       "      <th>post_link</th>\n",
       "      <th>post_date</th>\n",
       "      <th>post</th>\n",
       "      <th>user</th>\n",
       "      <th>user_verified_status</th>\n",
       "      <th>post_likes</th>\n",
       "      <th>post_verified_tags</th>\n",
       "      <th>post_unverified_tags</th>\n",
       "      <th>post_location</th>\n",
       "      <th>post_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [searched_for, post_link, post_date, post, user, user_verified_status, post_likes, post_verified_tags, post_unverified_tags, post_location, post_image]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "togetherness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'human_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b154cafda52b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhuman_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhumanexperience_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtogetherness_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconnection_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'human_df' is not defined"
     ]
    }
   ],
   "source": [
    "pd.concat([human_df,humanexperience_df,togetherness_df,connection_df]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "togetherness_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
